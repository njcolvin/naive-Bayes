{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "Let's make a naive Bayes classifier. We will run it on a collection of tweets and Reddit comments whose sentiment is either positive, neutral, or negative.\n",
    "\n",
    "First, let's load the data into training and test corpora. We'll simply combine, shuffle, and split the datasets into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def getsamples(lines: list):\n",
    "    n = len(lines)\n",
    "    i = 1\n",
    "    samples = []\n",
    "    while i < n:\n",
    "        line = lines[i]\n",
    "        xy = line.split(',')\n",
    "        x = xy[0].strip().split(' ')\n",
    "        if len(xy) == 1: # multiline tweet\n",
    "            line = lines[i][1:].rstrip() # remove double quote\n",
    "            i += 1\n",
    "            if len(line) == 0 and i < n:\n",
    "                # beginning of tweet so dont put space\n",
    "                line += lines[i].split(',')[0].rstrip()\n",
    "                i += 1\n",
    "            while i < n and len(lines[i].split(',')) == 1:\n",
    "                line += ' ' + lines[i].rstrip()\n",
    "                i += 1\n",
    "            xy = lines[i].split(',')\n",
    "            endsize = len(xy[0])\n",
    "            if endsize > 1: # ignore double quote i.e. endsize == 1\n",
    "                # remove double quote at the end\n",
    "                line += ' ' + xy[0][:endsize - 1] \n",
    "            x = line.split(' ')\n",
    "        y = int(xy[1].rstrip())\n",
    "        samples.append([x, y])\n",
    "        i += 1\n",
    "    return samples\n",
    "\n",
    "def getcorpus():\n",
    "    corpus = []\n",
    "    fnames = ['reddit', 'twitter']\n",
    "    ext = '.csv'\n",
    "    for fname in fnames:\n",
    "        with open(fname + ext, 'r', encoding='latin-1') as f:\n",
    "            corpus.extend(getsamples(f.readlines()))\n",
    "    return corpus\n",
    "\n",
    "corpus = getcorpus()\n",
    "shuffle(corpus) # shuffle so the test set isn't just twitter\n",
    "train = corpus[:int(len(corpus) * 0.8)]\n",
    "test = corpus[int(len(corpus) * 0.8):]\n",
    "classes = [-1, 0, 1] # negative, neutral, positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**\n",
    "\n",
    "Now we can define our naive Bayes classifier class. All we really need is a train function and a test function.\n",
    "\n",
    "To train, we calculate the probabilities of each class, P(c), and of every feature (in our case individual words) given each class, P(w|c), using MLE and counting.\n",
    "\n",
    "The prior probability of a class P(c) will be defined as the number of documents (tweets) in class c out of all available documents.\n",
    "\n",
    "The likelihood of a feature given a class P(w|c) will be defined as the number of occurrences of w in documents in class c out of the number of occurrences of all words in documents in class c.\n",
    "\n",
    "For a test sample, we find the probability of the sample document given each class, P(d|c), and return the argmax class as our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation Details**\n",
    "\n",
    "We'll need to import a log function since we would rather add up log probabilities than multiply (lots of near zero) probabilities.\n",
    "\n",
    "We'll also need the Counter class to get frequency counts from our training corpus. Specifically, we're going to use a hyperparameter `vocab_size` to set our features as the `vocab_size` most common words in the corpus. These are the most common words in the entire corpus, not just for a class. \n",
    "\n",
    "Notice the additional `smoothing` parameter. Since we are using individual words as features, there's a chance during training we find a word that doesn't show up in any documents for a class and gets assigned zero probability. The computer will not like taking log of zero, and we will not like the model. To avoid this, we simply add the value of `smoothing` to every word count to get rid of zero probabilities. Since we add the same amount to every count in the numerator and denominator, we maintain a valid probability distribution. This is called _Laplace smoothing_ or _Add-one smoothing_, and typically a value like 0.05 works better than 1 on smaller datasets because we don't want to assign too much probability to words that never occurred in a class.\n",
    "\n",
    "UPDATES:\n",
    "\n",
    "Included an unknown token `<unk>` as part of the vocabulary, so we can assign a probability to out of vocabulary (OOV) words during testing. Its probability for a given class is the sum of probabilities of all the OOV words in that class.\n",
    "\n",
    "Added an option to use a list of stopwords from NLTK. It's disabled by default because unless we somehow account for the effect of any stopwords in a sample we lose information and thus performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/njc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    UNK = '<unk>'\n",
    "\n",
    "    def __init__(self, C:list, vocab_size:int=5000, smoothing:float=0.05, ignore_stopwords:bool=False):\n",
    "        self.C = C\n",
    "        self.vocab_size = vocab_size\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_stopwords = ignore_stopwords\n",
    "\n",
    "    # ignore stopwords when building vocab\n",
    "    # returns the vocab list\n",
    "    def stopword_free_vocab(self, counts:Counter):\n",
    "        V = []\n",
    "        common_counts= counts.most_common()\n",
    "        size, i = 0, 0\n",
    "        while size < self.vocab_size:\n",
    "            while common_counts[i][0] in stopwords.words('english') or len(common_counts[i][0]) == 0:\n",
    "                i += 1\n",
    "            V.append(common_counts[i][0])\n",
    "            i += 1\n",
    "            size += 1\n",
    "        return V\n",
    "\n",
    "    # see most likely words in vocab\n",
    "    def common_class_words(self, class_index:int, num_words:int=10):\n",
    "        vocab_likelihoods = [row[class_index] for row in self.loglikelihood]\n",
    "        likely_word_indices = sorted(range(len(vocab_likelihoods)),\n",
    "                                     key=lambda idx: vocab_likelihoods[idx],\n",
    "                                     reverse=True)[:num_words]\n",
    "        for j in range(num_words):\n",
    "            w = self.V[likely_word_indices[j]] if likely_word_indices[j] < self.vocab_size else self.UNK\n",
    "            print(' (%s, %f)' % (w, vocab_likelihoods[j]))\n",
    "\n",
    "    def train(self, D:list):\n",
    "        # get counts from a single document of all samples\n",
    "        big = []\n",
    "        for i in range(len(D)):\n",
    "            big.extend(D[i][0])\n",
    "        counts = Counter(big)\n",
    "        \n",
    "        # build vocabulary\n",
    "        if self.ignore_stopwords:\n",
    "            self.V = self.stopword_free_vocab(counts)\n",
    "        else:\n",
    "            self.V = [tup[0] for tup in counts.most_common(self.vocab_size)]\n",
    "        \n",
    "        # iterate over the classes, compute P(c) and P(w|c)'s for each class\n",
    "        N_doc = len(D)\n",
    "        self.logpriors = []\n",
    "        self.bigdoc = []\n",
    "        self.loglikelihood = [[] for _ in range(self.vocab_size + 1)] # extra row for UNK\n",
    "        for i in range(len(self.C)):\n",
    "            c = self.C[i]\n",
    "            print('Training %d' % c)\n",
    "\n",
    "            # build class document and calculate P(c)\n",
    "            N_c = 0\n",
    "            self.bigdoc.append([])\n",
    "            for sample in D:\n",
    "                if sample[1] == c:\n",
    "                    N_c += 1\n",
    "                    self.bigdoc[i].extend(sample[0])\n",
    "            self.logpriors.append(log2(N_c / N_doc))\n",
    "            print(' Counted %d samples, log P(c) = %f. Counting word occurrences...' % (N_c, self.logpriors[i]))\n",
    "            \n",
    "            # get word counts from class doc\n",
    "            counter = Counter(self.bigdoc[i])\n",
    "            # smoothed occurrences, vocab_size + 1 for UNK\n",
    "            occurrences = sum(counter.values()) + self.smoothing * (self.vocab_size + 1)\n",
    "\n",
    "            print(' Counted %d occurrences. Computing log P(w|c)s...' % occurrences)\n",
    "            # get counts for every word in V\n",
    "            for j in range(self.vocab_size): \n",
    "                w = self.V[j]\n",
    "                c = counter[w]\n",
    "                self.loglikelihood[j].append(log2((c + self.smoothing) / occurrences))\n",
    "\n",
    "            # get sum of OOV word counts for UNK\n",
    "            unk_c = 0\n",
    "            for w, c in counter.most_common():\n",
    "                if w in self.V:\n",
    "                    continue\n",
    "                unk_c += c\n",
    "            self.loglikelihood[self.vocab_size].append(log2((unk_c + self.smoothing) / occurrences))\n",
    "\n",
    "            # print most common words\n",
    "            # self.common_class_words(i, 5)\n",
    "\n",
    "    # get probability of each class, choose argmax class as prediction\n",
    "    def test(self, testdoc):\n",
    "        max_prob, max_c = float('-inf'), None\n",
    "        for i in range(len(self.C)):\n",
    "            class_prob = self.logpriors[i]\n",
    "            for word in testdoc:\n",
    "                # don't give stopwords UNK probability if ignoring\n",
    "                if self.ignore_stopwords and word in stopwords.words('english'):\n",
    "                    continue\n",
    "                index = self.V.index(word) if word in self.V else self.vocab_size\n",
    "                class_prob += self.loglikelihood[index][i]\n",
    "            if class_prob > max_prob:\n",
    "                max_prob = class_prob\n",
    "                max_c = self.C[i]\n",
    "        return max_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be able to evaluate our predictions, so for now we will just compute the recall of each class with a simple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the true positive (prediction, not sentiment) rates for each class\n",
    "def evaluate(y, y_pred, tp, fp, tn, fn, tneu, fneu):\n",
    "    if y == -1:\n",
    "        if y_pred == -1:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    elif y == 0:\n",
    "        if y_pred == 0:\n",
    "            tneu += 1\n",
    "        else:\n",
    "            fneu += 1\n",
    "    else:\n",
    "        if y_pred == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    return (tp, fp, tn, fn, tneu, fneu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -1\n",
      " Counted 34804 samples, log P(c) = -2.196412. Counting word occurrences...\n",
      " Counted 908207 occurrences. Computing log P(w|c)s...\n",
      "Training 0\n",
      " Counted 54570 samples, log P(c) = -1.547557. Counting word occurrences...\n",
      " Counted 737328 occurrences. Computing log P(w|c)s...\n",
      "Training 1\n",
      " Counted 70146 samples, log P(c) = -1.185305. Counting word occurrences...\n",
      " Counted 1851252 occurrences. Computing log P(w|c)s...\n",
      "Testing 39880 samples\n",
      " 5000 samples, tpr 0.849655 tneur 0.774688 tnr 0.735654\n",
      " 10000 samples, tpr 0.845842 tneur 0.778969 tnr 0.722880\n",
      " 15000 samples, tpr 0.847724 tneur 0.776567 tnr 0.729147\n",
      " 20000 samples, tpr 0.849292 tneur 0.778674 tnr 0.728712\n",
      " 25000 samples, tpr 0.850263 tneur 0.777230 tnr 0.727173\n",
      " 30000 samples, tpr 0.850155 tneur 0.777898 tnr 0.725023\n",
      " 35000 samples, tpr 0.851736 tneur 0.777918 tnr 0.725493\n",
      "positive recall %: 85.145072\n",
      "neutral recall %: 77.702203\n",
      "negative recall %: 72.488934\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "model = NaiveBayesClassifier(classes, vocab_size)\n",
    "model.train(train)\n",
    "print('Testing %d samples' % len(test))\n",
    "tp, fp, tn, fn, tneu, fneu, count = 0, 0, 0, 0, 0, 0, 0\n",
    "for sample in test:\n",
    "    count += 1\n",
    "    y = sample[1]\n",
    "    y_pred = model.test(sample[0])\n",
    "    tp, fp, tn, fn, tneu, fneu = evaluate(y, y_pred, tp, fp, tn, fn, tneu, fneu)\n",
    "    if count % 5000 == 0:\n",
    "        print(' %d samples, tpr %f tneur %f tnr %f' % (count, (tp / (tp + fp)), (tneu / (tneu + fneu)), (tn / (tn + fn))))\n",
    "\n",
    "print('positive recall %%: %f' % (100 * tp / (tp + fp)))\n",
    "print('neutral recall %%: %f' % (100 * tneu / (tneu + fneu)))\n",
    "print('negative recall %%: %f' % (100 * tn / (tn + fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible Improvements**\n",
    "\n",
    "We could have different unknown tokens for nouns, verbs, and other parts of speech but that would require part of speech tagging.\n",
    "\n",
    "A naive Bayes classifier defined with individual words as features can be viewed as a set of unigram language models, one for each class. We can convert our classifier to use any natural number n-grams we want instead of unigrams. If so, we have more options for smoothing:\n",
    "\n",
    "1. Use *discounting* and *Katz backoff*, where we \"back off\" from an n-gram to (n-1)-gram if the probability of the n-gram is 0.\n",
    "2. *Interpolate* probabilities i.e. compute a weighted sum of each n-gram, (n-1)-gram, etc. probability such that the weights sum to 1.\n",
    "3. Use *Kneser-Ney smoothing* which computes the probability of a word as the probability of it appearing as a continuation in some novel context instead of just the count. This favors words that are \"friendlier\": those that appear after many different words.\n",
    "\n",
    "As mentioned earlier, just removing stopwords adversely affects performance. We can try to account for the lost meaning in each sample by transforming the remaining words somehow. For example, to handle removing stopwords that negate meaning like not, nor, or don't we can apply *negation tagging* by adding a prefix indicating negation to each affected word. The sentence \"I don't like pizza\" becomes \"I NOT_like NOT_pizza.\" The best way to implement this likely requires part of speech tagging.\n",
    "\n",
    "We used individual words as features, but we can add any kind of features we want. We could have binary features that merely indicates the presence of a word rather than its count, categorical features like parts of speech, or numerical like the number of words used from some predefined sentiment lexicon.\n",
    "\n",
    "Computing precision and F1 score along with recall would give a better indication of overall performance.\n",
    "\n",
    "Lastly, we could combine the Twitter and Reddit datasets and split our data into training, tuning, and test sets so we have chance to tune hyperparameters like vocab size or smoothing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
